{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Supply Chain Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data and display the first few rows of the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/raw_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLProv2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLProv2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/MLProv2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLProv2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/MLProv2/lib/python3.12/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data and display the first few rows of the dataset\n",
    "data = pd.read_csv('/Users/sk/Desktop/AIchemyLab/2_ML/ML_Reg_FMCG_Supply_Chain/data/raw_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 25,000 rows and 24 columns.\n",
    "\n",
    "Here are some key details:\n",
    "1. Data Types:\n",
    "2 float64 columns, \n",
    "14 int64 columns, \n",
    "8 object columns\n",
    "\n",
    "2. Columns with Missing Values:\n",
    "workers_num: 24,010 non-null, \n",
    "wh_est_year: 13,119 non-null, \n",
    "approved_wh_govt_certificate: 24,092 non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the missing values for each column in the dataset:\n",
    "1. workers_num: 990 missing values\n",
    "2. wh_est_year: 11,881 missing values\n",
    "3. approved_wh_govt_certificate: 908 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the dataset\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the overall theme and color palette\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(\"pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['product_wg_ton'], kde=True, color='dodgerblue')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Product Weight (in Tonne)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few insights from the plot:\n",
    "1. Shape of the Distribution: The distribution appears to be roughly symmetrical, indicating a fairly even spread of product weights across the dataset.\n",
    "2. Central Tendency: The peak of the distribution suggests that the most common product weights are around the central value.\n",
    "3. Spread: The product weights vary significantly, ranging from around 2000 tons to over 55000 tons.\n",
    "4. Density Plot: The KDE (Kernel Density Estimate) line provides a smooth estimate of the distribution, further highlighting the central tendency and spread.\n",
    "5. Outliers: There are no significant outliers visible, which suggests that the data does not have extreme values that might skew the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "data.drop(['Ware_house_ID', 'WH_Manager_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Distribution of the number of workers before imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['workers_num'].dropna(), kde=True, color='dodgerblue', bins=30)\n",
    "plt.title('Distribution of number of workers before Imputation')\n",
    "plt.xlabel('Number of Workers')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few insights from the plot:\n",
    "1. Shape of the Distribution: The distribution is roughly symmetrical with a slight skew to the right, indicating that most warehouses have a moderate number of workers.\n",
    "2. Central Tendency: The peak of the distribution suggests that the most common number of workers is around 30.\n",
    "3. Spread: The number of workers varies significantly, ranging from about 10 to 60, with a few outliers extending up to 98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for number of workers with the median\n",
    "data['workers_num'].fillna(data['workers_num'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: We use median imputation because it is more robust to outliers compared to mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values for approved_wh_govt_certificate\n",
    "print(\"Unique values in approved_wh_govt_certificate before imputation:\")\n",
    "print(data['approved_wh_govt_certificate'].unique())\n",
    "\n",
    "print(\"Mode of approved_wh_govt_certificate:\")\n",
    "print(data['approved_wh_govt_certificate'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for approved_wh_govt_certificate with new category\n",
    "data['approved_wh_govt_certificate'].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: The mode imputation would replace all missing values with the most frequent category. However, since approved_wh_govt_certificate is a categorical feature with specific meanings, filling with 'NA' allows us to retain the information that the certificate status is unknown rather than assuming it is the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation matrix calculation\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Skewness and Kurtosis\n",
    "skewness = numeric_data.skew()\n",
    "kurtosis = numeric_data.kurtosis()\n",
    "\n",
    "print(\"Skewness of the dataset:\\n\", skewness)\n",
    "print(\"\\nKurtosis of the dataset:\\n\", kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. Transport issues and number of competitors show a positive skew, indicating that higher values are less frequent.\n",
    "2. Flood impact and flood proof have high kurtosis values, indicating heavy tails or outliers in the distribution.\n",
    "3. Electric supply and distance from the hub show negative skew, indicating that lower values are less frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR\n",
    "Q1 = numeric_data.quantile(0.25)\n",
    "Q3 = numeric_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = numeric_data[(numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Filter rows with any outliers\n",
    "rows_with_outliers = data[outliers.any(axis=1)]\n",
    "\n",
    "print(\"Rows with outliers:\\n\", rows_with_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting outliers\n",
    "numeric_data1 = ['num_refill_req_l3m', 'transport_issue_l1y', 'Competitor_in_mkt',\n",
    "       'retail_shop_num', 'distributor_num', 'flood_impacted', 'flood_proof',\n",
    "       'electric_supply']\n",
    "\n",
    "# Create box plots for all numeric columns\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, column in enumerate(numeric_data1, 1):\n",
    "    plt.subplot(len(numeric_data1) // 2 + 1, 2, i)\n",
    "    sns.boxplot(data=data[column], color='dodgerblue')\n",
    "    plt.title(column)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting outliers\n",
    "numeric_data2 = ['dist_from_hub', 'workers_num', 'wh_est_year',\n",
    "       'storage_issue_reported_l3m', 'temp_reg_mach', 'wh_breakdown_l3m',\n",
    "       'govt_check_l3m', 'product_wg_ton']\n",
    "\n",
    "# Create box plots for all numeric columns\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, column in enumerate(numeric_data2, 1):\n",
    "    plt.subplot(len(numeric_data2) // 2 + 1, 2, i)\n",
    "    sns.boxplot(data=data[column], color='dodgerblue')\n",
    "    plt.title(column)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the numerical variables\n",
    "for column in numeric_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(numeric_data[column], kde=True, color='dodgerblue')\n",
    "    plt.title(f'Distribution of {column} Variable')\n",
    "    plt.xlabel(f'{column}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.pairplot(numeric_data, palette='dodgerblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copy for visualisation\n",
    "datavis = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Warehouse Capacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'WH_capacity_size' to categorical labels for plotting purposes\n",
    "datavis['WH_capacity_size'] = datavis['WH_capacity_size'].astype(pd.CategoricalDtype(categories=['Small', 'Mid', 'Large'], ordered=True))\n",
    "\n",
    "# Calculate the counts of each warehouse capacity size\n",
    "wh_cap_counts = datavis['WH_capacity_size'].value_counts().sort_index()\n",
    "# Find the maximum count of warehouses\n",
    "wh_cap_max_cat = wh_cap_counts.max()\n",
    "# Create custom color palette based on the maximum count\n",
    "wh_cap_palette = ['dodgerblue' if count == wh_cap_max_cat else 'lightskyblue' for count in wh_cap_counts]\n",
    "\n",
    "# Plot the number of warehouses by capacity size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=datavis, x='WH_capacity_size', hue='WH_capacity_size', palette=wh_cap_palette, legend=False)\n",
    "plt.title('Number of Warehouses by Warehouse Capacity Size')\n",
    "plt.xlabel('Warehouse Capacity Size')\n",
    "plt.ylabel('Number of Warehouses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. The 'Large' capacity size has the highest count among the three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts of each warehouse capacity size\n",
    "wh_cap_counts2 = datavis['zone'].value_counts().sort_index()\n",
    "# Find the maximum count of warehouses\n",
    "wh_cap_max_cat2 = wh_cap_counts2.max()\n",
    "# Create custom color palette based on the maximum count\n",
    "wh_cap_palette2 = ['dodgerblue' if count == wh_cap_max_cat2 else 'lightskyblue' for count in wh_cap_counts2]\n",
    "\n",
    "# Plot the number of warehouses by capacity size\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=datavis, x='zone', hue='zone', palette=wh_cap_palette2, legend=False)\n",
    "plt.title('Number of Warehouses by Zone')\n",
    "plt.xlabel('Zone')\n",
    "plt.ylabel('Number of Warehouses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of product shipped by warehouse capacity size\n",
    "wh_cap_prod_tot = datavis.groupby('WH_capacity_size', observed=False)['product_wg_ton'].sum().reset_index()\n",
    "\n",
    "# Find the maximum total weight of product shipped\n",
    "wh_cap_prod_max_cat = wh_cap_prod_tot['product_wg_ton'].max()\n",
    "\n",
    "# Create custom color palette based on the maximum total weight of product shipped\n",
    "wh_cap_prod_palette = ['dodgerblue' if value == wh_cap_prod_max_cat else 'lightskyblue' for value in wh_cap_prod_tot['product_wg_ton']]\n",
    "\n",
    "# Plot the total weight of product shipped by warehouse capacity size\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(wh_cap_prod_tot['WH_capacity_size'], wh_cap_prod_tot['product_wg_ton'], color=wh_cap_prod_palette)\n",
    "plt.title('Total Weight of Product Shipped by Warehouse Capacity Size')\n",
    "plt.xlabel('Warehouse Capacity Size')\n",
    "plt.ylabel('Total Weight of Product Shipped (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. The 'Large' capacity warehouses ship the highest total weight of products compared to 'Mid' and 'Small' capacity warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average weight of product shipped by warehouse capacity size\n",
    "wh_cap_prod_avg = datavis.groupby('WH_capacity_size', observed=False)['product_wg_ton'].mean().reset_index()\n",
    "\n",
    "# Find the maximum average weight\n",
    "wh_cap_prod_max2 = wh_cap_prod_avg['product_wg_ton'].max()\n",
    "\n",
    "# Create custom color palette based on the maximum average weight\n",
    "wh_cap_prod_palette2 = ['dodgerblue' if value == wh_cap_prod_max2 else 'lightskyblue' for value in wh_cap_prod_avg['product_wg_ton']]\n",
    "\n",
    "# Plot the average weight of product shipped by warehouse capacity size\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(wh_cap_prod_avg['WH_capacity_size'], wh_cap_prod_avg['product_wg_ton'], color=wh_cap_prod_palette2)\n",
    "plt.title('Average Weight of Product Shipped by Warehouse Capacity Size')\n",
    "plt.xlabel('Warehouse Capacity Size')\n",
    "plt.ylabel('Average Weight of Product Shipped (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. The 'Mid' capacity warehouses have the highest average weight of products shipped compared to 'Large' and 'Small' capacity warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean number of workers for each capacity size\n",
    "wh_cap_worker_avg = datavis.groupby('WH_capacity_size', observed=False)['workers_num'].mean().reset_index()\n",
    "\n",
    "# Find the maximum average number of workers\n",
    "wh_cap_worker_max = wh_cap_worker_avg['workers_num'].max()\n",
    "\n",
    "# Create custom color palette based on the maximum average number of workers\n",
    "wh_cap_worker_palette = ['dodgerblue' if value == wh_cap_worker_max else 'lightskyblue' for value in wh_cap_worker_avg['workers_num']]\n",
    "\n",
    "# Plot the mean number of workers by capacity size with customized colors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(wh_cap_worker_avg['WH_capacity_size'], wh_cap_worker_avg['workers_num'], color=wh_cap_worker_palette)\n",
    "plt.title('Average Number of Workers and Warehouse Capacity Size')\n",
    "plt.xlabel('Warehouse Capacity Size')\n",
    "plt.ylabel('Average Number of Workers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. The 'Small' capacity warehouses have the highest average number of workers compared to 'Mid' and 'Large' capacity warehouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Location and Accessibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'zone' and calculating the mean 'dist_from_hub'\n",
    "wh_loc_dist_avg = datavis.groupby('zone')['dist_from_hub'].mean().reset_index()\n",
    "\n",
    "# Find the maximum average distance from hub\n",
    "wh_loc_dist_max_cat = wh_loc_dist_avg['dist_from_hub'].max()\n",
    "\n",
    "# Create custom color palette based on the maximum average distance from hub\n",
    "wh_loc_dist_palette = ['dodgerblue' if value == wh_loc_dist_max_cat else 'lightskyblue' for value in wh_loc_dist_avg['dist_from_hub']]\n",
    "\n",
    "# Plotting with seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_loc_dist_avg, x='zone', y='dist_from_hub', hue='zone', palette=wh_loc_dist_palette, legend=False)\n",
    "plt.title('Average Distance from Hub by Zone')\n",
    "plt.xlabel('Zone')\n",
    "plt.ylabel('Average Distance from Hub')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Location_type' and calculating the mean 'dist_from_hub'\n",
    "wh_loc_loc_avg = datavis.groupby('Location_type')['dist_from_hub'].mean().reset_index()\n",
    "\n",
    "# Find the maximum average distance from hub\n",
    "wh_loc_loc_max_cat = wh_loc_loc_avg['dist_from_hub'].max()\n",
    "\n",
    "# Create custom color palette based on the maximum average distance from hub\n",
    "wh_loc_loc_palette = ['dodgerblue' if value == wh_loc_loc_max_cat else 'lightskyblue' for value in wh_loc_loc_avg['dist_from_hub']]\n",
    "\n",
    "# Plotting with seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_loc_loc_avg, x='Location_type', y='dist_from_hub', hue='Location_type', palette=wh_loc_loc_palette, legend=False)\n",
    "plt.title('Average Distance from Hub by Warehouse Location Type')\n",
    "plt.xlabel('Warehouse Location Type')\n",
    "plt.ylabel('Average Distance from Hub')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. Warehouses located in rural areas have a higher average distance from the hub compared to those in urban areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Warehouse Operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with known establishment years\n",
    "wh_op_est_filter = datavis.dropna(subset=['wh_est_year'])\n",
    "\n",
    "# Group by establishment year and calculate the mean of storage issues reported and warehouse breakdowns\n",
    "wh_op_str_avg = wh_op_est_filter.groupby('wh_est_year')['storage_issue_reported_l3m'].mean().reset_index()\n",
    "wh_op_break_avg = datavis.groupby('wh_est_year')['wh_breakdown_l3m'].mean().reset_index()\n",
    "\n",
    "# Plotting with seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=wh_op_str_avg, x='wh_est_year', y='storage_issue_reported_l3m', marker='o', color='dodgerblue', label='Storage Issues (last 3 months)')\n",
    "sns.lineplot(data=wh_op_break_avg, x='wh_est_year', y='wh_breakdown_l3m', marker='o', color='gold', label='Warehouse Breakdowns (last 3 months)')\n",
    "plt.title('Trends in Storage Issues and Warehouse Breakdowns Over Time')\n",
    "plt.xlabel('Year of Establishment')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average product weight shipped grouped by the number of workers\n",
    "wh_op_worker_wt_avg = data.groupby('workers_num')['product_wg_ton'].mean().reset_index()\n",
    "\n",
    "# Plotting with seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=wh_op_worker_wt_avg, x='workers_num', y='product_wg_ton', marker='o', color='dodgerblue')\n",
    "plt.title('Influence of Number of Workers on Product Weight Shipped')\n",
    "plt.xlabel('Number of Workers')\n",
    "plt.ylabel('Average Product Weight Shipped (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. Initially, as the number of workers increases, there seems to be an increase in the average weight shipped. After reaching a peak, the trend shows variability, indicating that other factors might also influence the product weight shipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total values for each issue by warehouse capacity size\n",
    "wh_op_issues_tot = datavis.groupby('WH_capacity_size', observed=False)[['storage_issue_reported_l3m', 'wh_breakdown_l3m', 'num_refill_req_l3m']].sum().reset_index()\n",
    "\n",
    "# Find the maximum value for each issue\n",
    "wh_op_strissue_max_cat = wh_op_issues_tot['storage_issue_reported_l3m'].max()\n",
    "wh_op_breakissue_max_cat = wh_op_issues_tot['wh_breakdown_l3m'].max()\n",
    "wh_op_refillissue_max_cat = wh_op_issues_tot['num_refill_req_l3m'].max()\n",
    "\n",
    "# Create color schemes for each issue\n",
    "wh_op_strissue_palette = ['dodgerblue' if value == wh_op_strissue_max_cat else 'lightskyblue' for value in wh_op_issues_tot['storage_issue_reported_l3m']]\n",
    "wh_op_breakissue_palette = ['gold' if value == wh_op_breakissue_max_cat else 'khaki' for value in wh_op_issues_tot['wh_breakdown_l3m']]\n",
    "wh_op_refillissue_palette = ['salmon' if value == wh_op_refillissue_max_cat else 'lightsalmon' for value in wh_op_issues_tot['num_refill_req_l3m']]\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.25\n",
    "bar_positions = np.arange(len(wh_op_issues_tot))\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "ax.bar(bar_positions, wh_op_issues_tot['storage_issue_reported_l3m'], width=bar_width, color=wh_op_strissue_palette, label='Storage Issues')\n",
    "ax.bar(bar_positions + bar_width, wh_op_issues_tot['wh_breakdown_l3m'], width=bar_width, color=wh_op_breakissue_palette, label='Breakdowns')\n",
    "ax.bar(bar_positions + 2 * bar_width, wh_op_issues_tot['num_refill_req_l3m'], width=bar_width, color=wh_op_refillissue_palette, label='Refill Required')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(bar_positions + bar_width)\n",
    "ax.set_xticklabels(wh_op_issues_tot['WH_capacity_size'])\n",
    "ax.set_title('Total Operational Issues by Warehouse Capacity Size')\n",
    "ax.set_xlabel('Warehouse Capacity Size')\n",
    "ax.set_ylabel('Total Number of Issues (last 3 months)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. Storage Issues: The 'Large' capacity warehouses have the highest total number of storage issues reported in the last three months.\n",
    "2. Breakdowns: Similarly, the 'Large' capacity warehouses also have the highest total number of warehouse breakdowns.\n",
    "3. Refill Requirements: The 'Large' capacity warehouses again show the highest total number of refills required, indicating more frequent refill needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total values for each issue by warehouse capacity size\n",
    "wh_op_issues_avg = datavis.groupby('WH_capacity_size', observed=False)[['storage_issue_reported_l3m', 'wh_breakdown_l3m', 'num_refill_req_l3m']].mean().reset_index()\n",
    "\n",
    "# Find the maximum value for each issue\n",
    "wh_op_strissue_max_cat2 = wh_op_issues_avg['storage_issue_reported_l3m'].max()\n",
    "wh_op_breakissue_max_cat2 = wh_op_issues_avg['wh_breakdown_l3m'].max()\n",
    "wh_op_refillissue_max_cat2 = wh_op_issues_avg['num_refill_req_l3m'].max()\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.25\n",
    "bar_positions = np.arange(len(wh_op_issues_avg))\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "ax.bar(bar_positions, wh_op_issues_avg['storage_issue_reported_l3m'], width=bar_width, color=wh_op_strissue_palette, label='Storage Issues')\n",
    "ax.bar(bar_positions + bar_width, wh_op_issues_avg['wh_breakdown_l3m'], width=bar_width, color=wh_op_breakissue_palette, label='Breakdowns')\n",
    "ax.bar(bar_positions + 2 * bar_width, wh_op_issues_avg['num_refill_req_l3m'], width=bar_width, color=wh_op_refillissue_palette, label='Refill Required')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(bar_positions + bar_width)\n",
    "ax.set_xticklabels(wh_op_issues_avg['WH_capacity_size'])\n",
    "ax.set_title('Total Operational Issues by Warehouse Capacity Size')\n",
    "ax.set_xlabel('Warehouse Capacity Size')\n",
    "ax.set_ylabel('Total Number of Issues (last 3 months)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iinsights:\n",
    "1. Storage Issues: The 'Large' capacity warehouses have the highest average number of storage issues reported in the last three months.\n",
    "2. Breakdowns: Similarly, the 'Large' capacity warehouses also have the highest average number of warehouse breakdowns.\n",
    "3. Refill Requirements: The 'Large' capacity warehouses again show the highest average number of refills required, indicating more frequent refill needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average storage issues reported by location type\n",
    "wh_op_loc_str_avg = data.groupby('Location_type')['storage_issue_reported_l3m'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_op_loc_str_max_cat = wh_op_loc_str_avg['storage_issue_reported_l3m'].max()\n",
    "\n",
    "# Create a color palette based on the maximum value\n",
    "wh_op_loc_str_palette = ['dodgerblue' if value == wh_op_loc_str_max_cat else 'lightskyblue' for value in wh_op_loc_str_avg['storage_issue_reported_l3m']]\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_op_loc_str_avg, x='Location_type', y='storage_issue_reported_l3m', hue='Location_type', palette=wh_op_loc_str_palette, legend=False)\n",
    "plt.title('Storage Issues Reported by Location Type')\n",
    "plt.xlabel('Location Type')\n",
    "plt.ylabel('Average Storage Issues Reported')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. Warehouses located in urban areas report a higher average number of storage issues compared to those in rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average storage issues reported by zone\n",
    "wh_op_zone_str_avg = data.groupby('zone')['storage_issue_reported_l3m'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_op_zone_str_max_cat = wh_op_zone_str_avg['storage_issue_reported_l3m'].max()\n",
    "\n",
    "# Create a color palette based on the maximum value\n",
    "wh_op_zone_str_palette = ['dodgerblue' if value == wh_op_zone_str_max_cat else 'lightskyblue' for value in wh_op_zone_str_avg['storage_issue_reported_l3m']]\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_op_zone_str_avg, x='zone', y='storage_issue_reported_l3m', hue='zone', palette=wh_op_zone_str_palette, legend=False)\n",
    "plt.title('Storage Issues Reported by Zone')\n",
    "plt.xlabel('Zone')\n",
    "plt.ylabel('Average Storage Issues Reported')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Logistics and Transportation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average transport issues reported by zone\n",
    "wh_logi_zone_avg = datavis.groupby('zone')['transport_issue_l1y'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_logi_zone_max_cat = wh_logi_zone_avg['transport_issue_l1y'].max()\n",
    "\n",
    "# Create color palette based on the maximum value\n",
    "wh_logi_zone_palette = ['dodgerblue' if value == wh_logi_zone_max_cat else 'lightskyblue' for value in wh_logi_zone_avg['transport_issue_l1y']]\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_logi_zone_avg, x='zone', y='transport_issue_l1y', hue='zone', palette=wh_logi_zone_palette, legend=False)\n",
    "plt.title('Transport Issues (last year) by Zone')\n",
    "plt.xlabel('Zone')\n",
    "plt.ylabel('Average Transport Issues (last year)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average transport issues reported by Location_type\n",
    "wh_logi_loc_avg = datavis.groupby('Location_type')['transport_issue_l1y'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_logi_loc_max_cat = wh_logi_loc_avg['transport_issue_l1y'].max()\n",
    "\n",
    "# Create color palette based on the maximum value\n",
    "wh_logi_loc_palette = ['dodgerblue' if value == wh_logi_loc_max_cat else 'lightskyblue' for value in wh_logi_loc_avg['transport_issue_l1y']]\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_logi_loc_avg, x='Location_type', y='transport_issue_l1y', hue='Location_type', palette=wh_logi_loc_palette, legend=False)\n",
    "plt.title('Transport Issues (last year) by Location Type')\n",
    "plt.xlabel('Location Type')\n",
    "plt.ylabel('Average Transport Issues (last year)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "1. The chart shows that warehouses in urban areas experience a higher average number of transport issues compared to those in rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for average transport issues by distance from hub\n",
    "wh_logi_dist_avg = datavis.groupby('transport_issue_l1y')['dist_from_hub'].mean().reset_index()\n",
    "\n",
    "# Define the conditional coloring logic\n",
    "wh_logi_dist_max_cat = wh_logi_dist_avg['dist_from_hub'].max()\n",
    "wh_logi_dist_palette = ['dodgerblue' if value == wh_logi_dist_max_cat else 'lightskyblue' for value in wh_logi_dist_avg['dist_from_hub']]\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_logi_dist_avg, y='dist_from_hub', x='transport_issue_l1y', hue='transport_issue_l1y', palette=wh_logi_dist_palette, legend=False)\n",
    "plt.title('Average Distance from Hub by Transport Issues (last year)')\n",
    "plt.ylabel('Average Distance from Hub')\n",
    "plt.xlabel('Transport Issues (last year)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for average transport issues by product weight\n",
    "wh_logi_prod_avg = datavis.groupby('transport_issue_l1y')['product_wg_ton'].mean().reset_index()\n",
    "\n",
    "# Define the conditional coloring logic\n",
    "wh_logi_prod_max_cat = wh_logi_prod_avg['product_wg_ton'].max()\n",
    "wh_logi_prod_palette = ['dodgerblue' if value == wh_logi_prod_max_cat else 'lightskyblue' for value in wh_logi_prod_avg['product_wg_ton']]\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_logi_prod_avg, y='product_wg_ton', x='transport_issue_l1y', hue='transport_issue_l1y', palette=wh_logi_prod_palette, legend=False)\n",
    "plt.title('Average Product Weight Shipped by Transport Issues (last year)')\n",
    "plt.ylabel('Average Product Weight Shipped (tons)')\n",
    "plt.xlabel('Transport Issues (last year)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. Competition and Market Factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges\n",
    "wh_mar_bin_edges = [0, 3, 6, 9, 12]\n",
    "\n",
    "# Define the bin labels\n",
    "wh_mar_bin_labels = ['0-3', '4-6', '7-9', '10-12']\n",
    "\n",
    "# Create a new column in the DataFrame to represent the bins\n",
    "datavis['Competitor_in_mkt_bins'] = pd.cut(datavis['Competitor_in_mkt'], bins=wh_mar_bin_edges, labels=wh_mar_bin_labels, right=False)\n",
    "\n",
    "# Bar plot for Competitors in Market vs. Number of Retail Shops, with hue representing the bins\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=datavis, x='retail_shop_num', y='zone', hue='Competitor_in_mkt_bins', estimator=sum, palette=\"Blues\")\n",
    "plt.title('Total Number of Retail Shops by Zone and Number of Competitors in Market')\n",
    "plt.xlabel('Total Number of Retail Shops')\n",
    "plt.ylabel('Zone')\n",
    "plt.legend(title='Number of Competitors in Market')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for Competitors in Market vs. Number of Retail Shops, with hue representing the bins\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=datavis, x='retail_shop_num', y='Location_type', hue='Competitor_in_mkt_bins', estimator=sum, palette=\"Blues\")\n",
    "plt.title('Total Number of Retail Shops by Warehouse Location Type and Number of Competitors in Market')\n",
    "plt.xlabel('Total Number of Retail Shops')\n",
    "plt.ylabel('Warehouse Location Type')\n",
    "plt.legend(title='Number of Competitors in Market')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitors' Influence on Warehouse Performance\n",
    "wh_mar_prod_tot = datavis.groupby('Competitor_in_mkt')['product_wg_ton'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=wh_mar_prod_tot, x='Competitor_in_mkt', y='product_wg_ton', marker='o', color='dodgerblue')\n",
    "plt.title('Product Weight Shipped by Number of Competitors')\n",
    "plt.xlabel('Number of Competitors')\n",
    "plt.ylabel('Product Weight Shipped (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitors' Influence on Warehouse Performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=datavis, x='Competitor_in_mkt', y='product_wg_ton', marker='o', color='dodgerblue')\n",
    "plt.title('Average Product Weight Shipped by Number of Competitors')\n",
    "plt.xlabel('Number of Competitors')\n",
    "plt.ylabel('Average Product Weight Shipped (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VI. Compliance and Regulatory Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=datavis, x='govt_check_l3m', y='zone', hue='approved_wh_govt_certificate', estimator='mean', palette='Blues')\n",
    "plt.title('Total Government Checks by Zone and Approved Govt Certificate')\n",
    "plt.xlabel('Total Government Checks (last 3 months)')\n",
    "plt.ylabel('Zone')\n",
    "plt.legend(title='Approved Govt Certificate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for government checks\n",
    "wh_comp_bin_edges = [0, 5, 10, 15, 20, 25, np.inf]\n",
    "wh_comp_bin_labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '>25']\n",
    "\n",
    "datavis['govt_check_bins'] = pd.cut(datavis['govt_check_l3m'], bins=wh_comp_bin_edges, labels=wh_comp_bin_labels, right=False)\n",
    "\n",
    "# Group by government check bins and calculate mean storage issues\n",
    "wh_comp_str_avg = datavis.groupby('govt_check_bins', observed=False)['storage_issue_reported_l3m'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_comp_str_max_cat = wh_comp_str_avg['storage_issue_reported_l3m'].max()\n",
    "\n",
    "# Create color palette based on the maximum value\n",
    "comp_str_palette = ['dodgerblue' if value == wh_comp_str_max_cat else 'lightskyblue' for value in wh_comp_str_avg['storage_issue_reported_l3m']]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_comp_str_avg, x='govt_check_bins', y='storage_issue_reported_l3m', hue='govt_check_bins', palette=comp_str_palette, legend=False)\n",
    "plt.title('Average Storage Issues by Government Checks')\n",
    "plt.xlabel('Government Checks (last 3 months)')\n",
    "plt.ylabel('Average Storage Issues Reported (last 3 months)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by government check bins and calculate mean storage issues\n",
    "wh_comp_break_avg = datavis.groupby('govt_check_bins', observed=False)['wh_breakdown_l3m'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_comp_break_max_cat = wh_comp_break_avg['wh_breakdown_l3m'].max()\n",
    "\n",
    "# Create color palette based on the maximum value\n",
    "comp_break_palette = ['dodgerblue' if value == wh_comp_break_max_cat else 'lightskyblue' for value in wh_comp_break_avg['wh_breakdown_l3m']]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wh_comp_break_avg, x='govt_check_bins', y='wh_breakdown_l3m', hue='govt_check_bins', palette=comp_break_palette, legend=False)\n",
    "plt.title('Average Warehouse Breakdowns by Government Checks')\n",
    "plt.xlabel('Government Checks (last 3 months)')\n",
    "plt.ylabel('Average Warehouse Breakdowns Reported (last 3 months)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VII. Infrastructure and Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total values for each issue by warehouse capacity size\n",
    "wh_inf_issues_avg = datavis.groupby('flood_impacted', observed=False)[['storage_issue_reported_l3m', 'wh_breakdown_l3m']].mean().reset_index()\n",
    "\n",
    "# Find the maximum value for each issue\n",
    "wh_inf_strissue_max_cat = wh_inf_issues_avg['storage_issue_reported_l3m'].max()\n",
    "wh_inf_breakissue_max_cat = wh_inf_issues_avg['wh_breakdown_l3m'].max()\n",
    "\n",
    "# Create color schemes for each issue\n",
    "wh_inf_strissue_palette = ['dodgerblue' if value == wh_inf_strissue_max_cat else 'lightskyblue' for value in wh_inf_issues_avg['storage_issue_reported_l3m']]\n",
    "wh_inf_breakissue_palette = ['gold' if value == wh_inf_breakissue_max_cat else 'khaki' for value in wh_inf_issues_avg['wh_breakdown_l3m']]\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.4\n",
    "bar_positions = np.arange(len(wh_inf_issues_avg))\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "ax.bar(bar_positions, wh_inf_issues_avg['storage_issue_reported_l3m'], width=bar_width, color=wh_inf_strissue_palette, label='Storage Issues')\n",
    "ax.bar(bar_positions + bar_width, wh_inf_issues_avg['wh_breakdown_l3m'], width=bar_width, color=wh_inf_breakissue_palette, label='Breakdowns')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(bar_positions + bar_width/2)\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "ax.set_title('Total Operational Issues by Flood Impact')\n",
    "ax.set_xlabel('Flood Impact')\n",
    "ax.set_ylabel('Average Number of Issues (last 3 months)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total values for each issue by warehouse capacity size\n",
    "wh_inf_issues_avg2 = datavis.groupby('electric_supply', observed=False)[['storage_issue_reported_l3m', 'wh_breakdown_l3m']].mean().reset_index()\n",
    "\n",
    "# Find the maximum value for each issue\n",
    "wh_inf_strissue_max_cat2 = wh_inf_issues_avg2['storage_issue_reported_l3m'].max()\n",
    "wh_inf_breakissue_max_cat2 = wh_inf_issues_avg2['wh_breakdown_l3m'].max()\n",
    "\n",
    "# Create color schemes for each issue\n",
    "wh_inf_strissue_palette2 = ['dodgerblue' if value == wh_inf_strissue_max_cat2 else 'lightskyblue' for value in wh_inf_issues_avg2['storage_issue_reported_l3m']]\n",
    "wh_inf_breakissue_palette2 = ['gold' if value == wh_inf_breakissue_max_cat2 else 'khaki' for value in wh_inf_issues_avg2['wh_breakdown_l3m']]\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.4\n",
    "bar_positions = np.arange(len(wh_inf_issues_avg2))\n",
    "\n",
    "# Plot each bar group separately to apply different colors\n",
    "ax.bar(bar_positions, wh_inf_issues_avg2['storage_issue_reported_l3m'], width=bar_width, color=wh_inf_strissue_palette2, label='Storage Issues')\n",
    "ax.bar(bar_positions + bar_width, wh_inf_issues_avg2['wh_breakdown_l3m'], width=bar_width, color=wh_inf_breakissue_palette2, label='Breakdowns')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(bar_positions + bar_width/2)\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "ax.set_title('Total Operational Issues by Availability of Electricity')\n",
    "ax.set_xlabel('Availability of Electricity')\n",
    "ax.set_ylabel('Average Number of Issues (last 3 months)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by government check bins and calculate mean storage issues\n",
    "wh_inf_prod_avg = datavis.groupby('temp_reg_mach', observed=False)['product_wg_ton'].mean().reset_index()\n",
    "\n",
    "# Determine the maximum value for coloring\n",
    "wh_inf_prod_max_cat = wh_inf_prod_avg['product_wg_ton'].max()\n",
    "\n",
    "# Create color palette based on the maximum value\n",
    "wh_inf_prod_palette = ['dodgerblue' if value == wh_inf_prod_max_cat else 'lightskyblue' for value in wh_inf_prod_avg['product_wg_ton']]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(data=wh_inf_prod_avg, x='temp_reg_mach', y='product_wg_ton', hue='temp_reg_mach', palette=wh_inf_prod_palette, legend=False)\n",
    "plt.title('Average Product Weight Shipped by Temperature Regulation Machinery')\n",
    "plt.xlabel('Temperature Regulation Machinery')\n",
    "plt.ylabel('Average Product Weight Shipped (tons)')\n",
    "ax.set_xticks(range(len(wh_inf_prod_avg)))\n",
    "ax.set_xticklabels(['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIII. Historical Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical trend analysis (for warehouses with known establishment years)\n",
    "wh_est_year_trend = datavis.dropna(subset=['wh_est_year']).groupby('wh_est_year')['product_wg_ton'].sum()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=wh_est_year_trend, marker='o', color='dodgerblue')\n",
    "plt.title('Historical Trend of Product Weight Shipped')\n",
    "plt.xlabel('Year of Establishment')\n",
    "plt.ylabel('Total Product Weight (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical trend analysis (for warehouses with known establishment years)\n",
    "wh_est_year_trend2 = datavis.dropna(subset=['wh_est_year']).groupby('wh_est_year')['product_wg_ton'].mean()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=wh_est_year_trend2, marker='o', color='dodgerblue')\n",
    "plt.title('Historical Trend of Product Weight Shipped')\n",
    "plt.xlabel('Year of Establishment')\n",
    "plt.ylabel('Average Product Weight (tons)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering & Baseline Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indicator variable for wh_est_year missingness\n",
    "data['wh_est_year_missing'] = data['wh_est_year'].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis before proceeding with missing value handling and model building.\n",
    "# Use a placeholder value that is clearly outside the valid range for 'wh_est_year'\n",
    "year_placeholder = -1\n",
    "data['wh_est_year'] = data['wh_est_year'].fillna(year_placeholder)\n",
    "\n",
    "features = data.drop(columns=['product_wg_ton'])\n",
    "target = data['product_wg_ton']\n",
    "features_encoded = pd.get_dummies(features, drop_first=True)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(features_encoded, target)\n",
    "\n",
    "feature_importances = rf.feature_importances_\n",
    "feature_names = features_encoded.columns\n",
    "\n",
    "feature_importance_data = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "feature_importance_data = feature_importance_data.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize most important features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_data.head(15), color='dodgerblue', legend=False)\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating base model v1 (Single Variable Linear Regression)\n",
    "# Select the most important feature and the target variable\n",
    "base_X = data[['storage_issue_reported_l3m']]\n",
    "base_y = data['product_wg_ton']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "base_X_train, base_X_test, base_y_train, base_y_test = train_test_split(base_X, base_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "base_sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "base_X_train_scaled = base_sc.fit_transform(base_X_train)\n",
    "\n",
    "# Transform the test data using the already fitted scaler\n",
    "base_X_test_scaled = base_sc.transform(base_X_test)\n",
    " \n",
    "# Create and fit the model\n",
    "base_model = LinearRegression()\n",
    "base_model.fit(base_X_train_scaled, base_y_train)\n",
    "\n",
    "# Make predictions\n",
    "base_y_pred = base_model.predict(base_X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "base_mae = mean_absolute_error(base_y_test, base_y_pred)\n",
    "base_mse = mean_squared_error(base_y_test, base_y_pred)\n",
    "base_r2 = r2_score(base_y_test, base_y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error: {base_mae}')\n",
    "print(f'Mean Squared Error: {base_mse}')\n",
    "print(f'R-squared: {base_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients and intercept from the model\n",
    "beta = base_model.coef_[0]\n",
    "beta_0 = base_model.intercept_\n",
    "\n",
    "print(\"Model Coefficients:\", beta)\n",
    "print(\"Model Intercept:\", beta_0)\n",
    "\n",
    "# Mean and standard deviation used for scaling\n",
    "mu = base_sc.mean_\n",
    "sigma = base_sc.scale_\n",
    "\n",
    "# Calculate new coefficients and intercept\n",
    "alpha_0 = beta_0 - np.sum(beta * (mu / sigma))\n",
    "alpha = beta / sigma\n",
    "\n",
    "print(\"Reverse Scaled Intercept (alpha_0):\", alpha_0)\n",
    "print(\"Reverse Scaled Coefficients (alpha):\", alpha)\n",
    "\n",
    "# Final equation\n",
    "equation = f\"y = {alpha_0} + ({alpha[0]}) * x1\"\n",
    "print(\"Base Equation:\", equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse scaled coefficients and intercept\n",
    "alpha_0 = 689.4218640991749\n",
    "alpha_1 = 1249.9555675709278\n",
    "\n",
    "# Generate a range of values for x1\n",
    "x1_range = np.linspace(0, 50)\n",
    "\n",
    "# Calculate corresponding y values\n",
    "y = alpha_0 + alpha_1 * x1_range\n",
    "\n",
    "# Plot the equation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x1_range, y, label=f'y = {alpha_0:.2f} + {alpha_1:.2f} * x1', color='dodgerblue')\n",
    "plt.xlabel('Predictor')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Base Linear Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "# Plot 1: Actual vs. Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(base_y_test, base_y_pred, color='dodgerblue', label='Actual vs Predicted')\n",
    "plt.plot([base_y_test.min(), base_y_test.max()], [base_y_test.min(), base_y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Linear Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Feature vs. Actual and Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(base_X_test, base_y_test, color='dodgerblue', label='Actual')\n",
    "plt.scatter(base_X_test, base_y_pred, color='gold', label='Predicted')\n",
    "plt.xlabel('Storage Issues Reported (Last 3 Months)')\n",
    "plt.ylabel('Product Weight (ton)')\n",
    "plt.title('Linear Regression: Feature vs. Actual and Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating base model v2 (Linear Regression)\n",
    "# Select the top 5 features and the target variable\n",
    "base2_data_encoded = pd.get_dummies(data, drop_first=True)\n",
    "base2_selected_features = ['storage_issue_reported_l3m', 'approved_wh_govt_certificate_B', \n",
    "                           'approved_wh_govt_certificate_B+']\n",
    "\n",
    "base2_X = base2_data_encoded[base2_selected_features]\n",
    "base2_y = data['product_wg_ton']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "base2_X_train, base2_X_test, base2_y_train, base2_y_test = train_test_split(base2_X, base2_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "base2_sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "base2_X_train_scaled = base2_sc.fit_transform(base2_X_train)\n",
    "\n",
    "# Transform the test data using the already fitted scaler\n",
    "base2_X_test_scaled = base2_sc.transform(base2_X_test)\n",
    "\n",
    "# Create and fit the model\n",
    "base2_model = LinearRegression()\n",
    "base2_model.fit(base2_X_train_scaled, base2_y_train)\n",
    "\n",
    "# Make predictions\n",
    "base2_y_pred = base2_model.predict(base2_X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "base2_mae = mean_absolute_error(base2_y_test, base2_y_pred)\n",
    "base2_mse = mean_squared_error(base2_y_test, base2_y_pred)\n",
    "base2_r2 = r2_score(base2_y_test, base2_y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error: {base2_mae}')\n",
    "print(f'Mean Squared Error: {base2_mse}')\n",
    "print(f'R-squared: {base2_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "# Plot 1: Actual vs. Predicted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(base2_y_test, base2_y_pred, color='dodgerblue', label='Actual vs Predicted')\n",
    "plt.plot([base2_y_test.min(), base2_y_test.max()], [base2_y_test.min(), base2_y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Linear Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Feature vs. Actual and Predicted\n",
    "# Assuming 'storage_issue_reported_l3m' is the first feature in base2_X_test\n",
    "feature_index = 0\n",
    "feature_name = base2_selected_features[feature_index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(base2_X_test.iloc[:, feature_index], base2_y_test, color='dodgerblue', label='Actual')\n",
    "plt.scatter(base2_X_test.iloc[:, feature_index], base2_y_pred, color='gold', label='Predicted')\n",
    "plt.xlabel(feature_name)\n",
    "plt.ylabel('Product Weight (ton)')\n",
    "plt.title('Linear Regression: Feature vs. Actual and Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping column with 48% missing data\n",
    "df = data.drop(['wh_est_year', 'wh_est_year_missing'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe as a CSV file\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "print(\"Dataframe saved as 'processed_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target variable\n",
    "X = df.drop(columns=['product_wg_ton'])\n",
    "y = df['product_wg_ton']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit the RFE model\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=11)\n",
    "rfe = rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select the most important features\n",
    "selected_features = X_encoded.columns[rfe.support_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Fit the model with the selected features\n",
    "X_train_selected = X_train_scaled[:, rfe.support_]\n",
    "X_test_selected = X_test_scaled[:, rfe.support_]\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='dodgerblue', label='Actual vs Predicted')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('RFE Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column indices for the selected features\n",
    "selected_features = ['storage_issue_reported_l3m', 'approved_wh_govt_certificate_B', 'approved_wh_govt_certificate_B+']\n",
    "selected_indices = [X_train.columns.get_loc(feature) for feature in selected_features]\n",
    "\n",
    "# Prepare the data with selected features\n",
    "X_train_selected = X_train_scaled[:, selected_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_indices]\n",
    "\n",
    "# Function to evaluate model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    mae_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error')\n",
    "    mse_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "    \n",
    "    return {\n",
    "        'MAE': -mae_scores.mean(),\n",
    "        'MSE': -mse_scores.mean(),\n",
    "        'R2': r2_scores.mean()\n",
    "    }\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_model(model, X_train_selected, y_train)\n",
    "\n",
    "# Display results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model} - MAE: {metrics['MAE']}, MSE: {metrics['MSE']}, R2: {metrics['R2']}\")\n",
    "\n",
    "# Plot results\n",
    "metrics_names = ['MAE', 'MSE', 'R2']\n",
    "n_models = len(models)\n",
    "x = np.arange(n_models)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics_names):\n",
    "    values = [results[model][metric] for model in models]\n",
    "    axs[i].bar(x, values)\n",
    "    axs[i].set_xticks(x)\n",
    "    axs[i].set_xticklabels(models.keys())\n",
    "    axs[i].set_title(metric)\n",
    "    axs[i].set_xlabel('Model')\n",
    "    axs[i].set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building final model\n",
    "# Perform one-hot encoding\n",
    "lr_df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "lr_selected_features = ['storage_issue_reported_l3m', 'approved_wh_govt_certificate_B', \n",
    "                        'approved_wh_govt_certificate_B+']\n",
    "\n",
    "lr_X = lr_df_encoded[lr_selected_features]\n",
    "lr_y = df['product_wg_ton']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "lr_X_train, lr_X_test, lr_y_train, lr_y_test = train_test_split(lr_X, lr_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "lr_sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "lr_X_train_scaled = lr_sc.fit_transform(lr_X_train)\n",
    "\n",
    "# Transform the test data using the already fitted scaler\n",
    "lr_X_test_scaled = lr_sc.transform(lr_X_test)\n",
    "\n",
    "# Hyperparameter tuning and cross-validation\n",
    "param_grid = {'fit_intercept': [True, False]}\n",
    "grid_search = GridSearchCV(LinearRegression(), param_grid, cv=10, scoring='r2')\n",
    "grid_search.fit(lr_X_train_scaled, lr_y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Make predictions using the best model\n",
    "lr_y_pred = best_model.predict(lr_X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_mae = mean_absolute_error(lr_y_test, lr_y_pred)\n",
    "lr_mse = mean_squared_error(lr_y_test, lr_y_pred)\n",
    "lr_r2 = r2_score(lr_y_test, lr_y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error: {lr_mae}')\n",
    "print(f'Mean Squared Error: {lr_mse}')\n",
    "print(f'R-squared: {lr_r2}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(lr_y_test, lr_y_pred, color='dodgerblue')\n",
    "plt.plot([lr_y_test.min(), lr_y_test.max()], [lr_y_test.min(), lr_y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = lr_y_test - lr_y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(lr_y_pred, residuals, color='dodgerblue')\n",
    "plt.hlines(y=0, xmin=lr_y_pred.min(), xmax=lr_y_pred.max(), colors='red', linestyles='dashed')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and scale (standard deviation) from the scaler\n",
    "best_mu = lr_sc.mean_\n",
    "best_sigma = lr_sc.scale_\n",
    "\n",
    "# Extract coefficients and intercept from the best model\n",
    "best_coef_1 = best_model.coef_[0]\n",
    "best_coef_2 = best_model.coef_[1]\n",
    "best_coef_3 = best_model.coef_[2]\n",
    "best_intercept = best_model.intercept_\n",
    "\n",
    "print(\"Model Coefficient 1:\", best_coef_1)\n",
    "print(\"Model Coefficient 2:\", best_coef_1)\n",
    "print(\"Model Coefficient 3:\", best_coef_1)\n",
    "print(\"Model Intercept:\", best_intercept)\n",
    "\n",
    "# Calculate new coefficients and intercept considering the scaling\n",
    "best_alpha_0 = best_intercept - (best_coef_1 * best_mu[0] / best_sigma[0]) - (best_coef_2 * best_mu[1] / best_sigma[1]) - (best_coef_3 * best_mu[2] / best_sigma[2])\n",
    "best_alpha_1 = best_coef_1 / best_sigma[0]\n",
    "best_alpha_2 = best_coef_2 / best_sigma[1]\n",
    "best_alpha_3 = best_coef_3 / best_sigma[2]\n",
    "\n",
    "print(\"Reverse Scaled Intercept (alpha_0):\", best_alpha_0)\n",
    "print(\"Reverse Scaled Coefficient 1 (alpha_1):\", best_alpha_1)\n",
    "print(\"Reverse Scaled Coefficient 2 (alpha_2):\", best_alpha_2)\n",
    "print(\"Reverse Scaled Coefficient 3 (alpha_3):\", best_alpha_3)\n",
    "\n",
    "# Final equation\n",
    "best_equation = f\"y = {best_alpha_0} + ({best_alpha_1}) * x1 + ({best_alpha_2}) * x2 + ({best_alpha_3}) * x3\"\n",
    "print(\"Final Equation:\", best_equation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
